---
title: "Trabajo practico 1"
subtitle: "Parte 1"
author: "Grupo ....."
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Librerias

```{r}
library(FactoMineR)
library(kableExtra)
library(tidyverse)
```

## Datos

```{r}
datos <- read_csv2(here::here("data", "raw", "DAMASCO.csv"))
```

Inspeccionamos con `glimpse(datos)`

```{r}
glimpse(datos)
```

**A.** Cuáles son los valores de $n$ y $p$? Cuanto vale y que indica el valor 
$x_{32}$ ? Y el vector $\mathbf{x}_6$ ?

El valor de $n$ es `r nrow(datos)` y el valor de $p$ es `r ncol(datos)`. El
valor de $x_{32}$ es `r datos[[3, 2]]` e indica el tamaño de la flor de la 
observacion 3. El vector $\mathbf{x}_6$ indica la relacion entre el ancho y largo de la 
hoja.

**B.** Cómo clasificaría las variables sobre las que se está trabajando ?

Las variables con las que se esta trabajando son de tipo continuo en todos los 
casos. Mas aun, todas estan medidas en escala de intervalo.

**C.** Encuentre el vector de medias y matriz de varianzas-covariancias 
asociados a la tabla de datos.

Las medias son:

```{r}
datos %>% 
    select(-VAR) %>% 
    summarise_all(mean) %>% 
    round(1) %>% 
    kable() %>% 
    kable_styling(font_size = 12) %>% 
    kable_classic_2()
```

La matriz de covarianza es:

```{r, out.width=.5}
datos %>% 
    select(-VAR) %>%  
    cov() %>%  
    round(1) %>% 
    kable() %>% 
    kable_styling(font_size = 12) %>% 
    kable_classic_2() 
```

**D.** Podría decir cuál y cuáles variables son las más dispersas ?

Utilizando el coeficiente de variación, podemos decir que las variables más 
dispersas, en orden decreciente, son:

```{r}
datos %>% select(-VAR) %>%
    summarise_all(function(x) sd(x) / mean(x)) %>% 
    pivot_longer(cols = everything(), names_to = "VAR", values_to = "CV") %>% 
    arrange(desc(CV)) %>% 
    mutate(CV = round(CV, 1)) %>%
    kable() %>% 
    kable_styling(font_size = 12, full_width = FALSE) %>% 
    kable_classic_2()
```

**E.** Estandarice las variables por media y desvío. Ahora puede responder al 
inciso (d) ?

```{r}
# No utilizamos la primer columna
datos_std = scale(datos[-1])
```

Como son variables estandarizadas, el coeficiente de variacion no existe,
pero todas tienen desvio estandar 1 (ya que la media es 0)

```{r}
datos_std %>%
    as_tibble() %>% 
    summarise_all(function(x) sd(x, na.rm=TRUE) /  mean(x, na.rm=TRUE)) %>% 
    round(1)
```

**F.** Halle la matriz de correlación. Que variables son las más relacionadas?

La matriz de correlación es:

```{r}
datos %>% 
    select(-VAR) %>%
    cor() %>% 
    round(1)  %>% 
    kable() %>% 
    kable_styling(font_size = 12)%>% 
    kable_classic_2()
```

A continuación una representacion grafica de la matriz de correlacion, la cual permite
identificar de forma mas sencilla las variables mas relacionadas:      

```{r}
datos%>%
    select(-VAR) %>%
    cor() %>%
    corrplot::corrplot(
      method = "circle", 
      type = "lower", 
      tl.srt = 15
    )
```

**G.** Pueden dividirse las variables en subgrupos, de modo que las variables 
dentro de un mismo subgrupo tengan elevadas correlaciones entre sí y que las que
se encuentren en subgrupos diferentes tengan bajas correlaciones? Si es así, 
cuáles variables quedan en cada uno de los subgrupos?

* Los subgrupos de variables con altas correlaciones son los siguientes: 

1. Peso, longitud, ancho y espesor del fruto (caracteristicas del fruto)
1. Peso, longitud y ancho del endocarpio (caracteristicas del endocarpio)
1. Tamaño de la flor, longitu y ancho del petalo (caracteristicas de la flor)

* El subgrupo de variables con bajas correlaciones son los siguientes: 

1. Superficie de la hoja, relacion entre peciolo-limbo y relacion entre longitud
y ancho de la hoja (caracteristicas de la hoja); tamaño de la flor, longitud y 
ancho del petalo (caracteristicas de la flor)

Cabe destacar que la superficie de la hoja y relacion peciolo-limbo presentan
correlacion muy baja o nula con cualquiera de las otras variables.

**H.** Encuentre la matriz que mide el grado de similaridad entre las variedades
en función de la distancia euclídea calculada sobre los datos originales.


```{r}
dist <- data.frame(datos[-1])
rownames(dist) <- datos$VAR

matriz_distancia <- dist %>% 
    dist(method = "euclidean", diag = TRUE, upper = TRUE) %>% 
    as.matrix()

 matriz_distancia %>% 
    round(1) %>% 
    kable() %>% 
    kable_styling() %>% 
    kable_classic_2()%>%
    scroll_box(height = "300px", width = "100%")
```

**I.** Podría decir cuales son los tres pares de variedades que presentan mayor 
semejanza?

Los pares de variedades que presentan mayor semejanza son:

```{r}
matriz_distancia_df <- data.frame(
  "var1" = rep(colnames(matriz_distancia), each = ncol(matriz_distancia)), 
  "var2" = rep(colnames(matriz_distancia), ncol(matriz_distancia)),
  "distancia" = c(matriz_distancia)
)

matriz_distancia_df %>%
    filter(
      rep(
          1:ncol(matriz_distancia),
          each = ncol(matriz_distancia)
      ) > 
      rep(
          1:ncol(matriz_distancia),
          times = ncol(matriz_distancia)
      )
  ) %>%
  arrange(distancia) %>%
  head(3) %>%
  kable(col.names = c("Variedad 1", "Variedad 2", "Distancia")) %>% 
  kable_styling(font_size = 12, full_width = FALSE) %>% 
  kable_classic_2()
```


**J.** Repita lo realizado en el inciso (h) pero sobre las variables 
estandarizadas por media y desvío estándar. Son las mismas las tres variedades 
más parecidas? Comente al respecto

```{r, class.source="fold-hide"}
dist_std <- data.frame(scale(datos[-1]))
rownames(dist_std) <- datos$VAR

matriz_distancia_std <- dist_std %>% 
    dist(method = "euclidean", diag = TRUE, upper = TRUE) %>% 
    as.matrix()

matriz_distancia_std %>%
    round(1) %>% 
    kable() %>% 
    kable_styling() %>% 
    kable_classic_2()%>%
    scroll_box(height = "300px", width = "100%")

```

Con los datos estandarizados, los pares de variedades mas parecidas son:

```{r}
matriz_distancia_std_df <- data.frame(
  "var1" = rep(colnames(matriz_distancia_std), each = ncol(matriz_distancia_std)), 
  "var2" = rep(colnames(matriz_distancia_std), ncol(matriz_distancia_std)),
  "distancia" = c(matriz_distancia_std)
)


matriz_distancia_std_df %>%
    filter(
      rep(
          1:ncol(matriz_distancia_std),
          each = ncol(matriz_distancia_std)
      ) > 
      rep(
          1:ncol(matriz_distancia_std),
          times = ncol(matriz_distancia_std)
      )
  ) %>%
  arrange(distancia) %>%
  head(3) %>%
  kable(col.names = c("Variedad 1", "Variedad 2", "Distancia")) %>% 
  kable_styling(font_size = 12, full_width = FALSE) %>% 
  kable_classic_2()

```

Podemos ver que los tres pares de variedades mas parecidas son distintos a los
que vimos en el inciso anterior donde utilizamos los datos sin estandarizar. 
Esto sucede porque las variables estan medidas en diferentes unidades de 
medicion, y al utilizar las variables sin estandarizar se le da mayor peso a 
las que tienen una variabilidad mayor valor en la escala de medida original.

**K.** Mida el grado de concordancia entre ambas matrices de distancia.

```{r}
matriz_distancia_dist <- as.dist(matriz_distancia)
matriz_distancia_std_dist <- as.dist(matriz_distancia_std)
conc <- cor(matriz_distancia_dist, matriz_distancia_std_dist) %>%
      as_tibble() %>%
      round(2) %>%
      rename(CONCORDANCIA = value)
```

El grado de concordancia entre las matrices de distancias es: `r conc `.

A continuacion se realiza una representacion grafica:

```{r}
plot(
  matriz_distancia_dist, 
  matriz_distancia_std_dist, 
  ylab = "Distancias euclideas estandarizadas", 
  xlab = "Distancis euclideas", 
  main = "Concordancia entre las matrices de distancias euclideas"
)
```

**L.** Realice un Análisis de Componentes Principales utilizando de la matriz de
correlaciones.

Los autovalores obtenidos son los siguientes:

```{r}
datos_std <- data.frame(datos_std)
rownames(datos_std) <- datos$VAR

pca <- PCA(datos_std, ncp = 2, graph = FALSE)

pca$eig %>%
  data.frame() %>%
  rename(
    "Autovalores" = "eigenvalue", 
    "% Varianza" = "percentage.of.variance", 
    "% Varianza acumulada" = "cumulative.percentage.of.variance" 
  ) %>%
  kable() %>%
  kable_styling(full_width = FALSE) %>%
  kable_classic_2()
```

Tambien pueden observarse graficamente: 

```{r}
# Agregar linea que acumula.
prop_table = prop.table(pca$eig[, 1])
barplot(prop_table, main = "AUTOVALORES", names.arg = 1:nrow(pca$eig))
#lines(c(1, 13), c(0, 0.5))
```

Y los autovectores (cargas asociadas a cada componente) son: 

```{r}
pca$var$coord %>%
  kable() %>%
  kable_styling(full_width = FALSE) %>%
  kable_classic_2()
```

**M.** Analice los porcentaje de variabilidad explicada por los primeros ejes 
principales.

Observando los autovalores y el porcentaje de varianza explicada de cada uno, 
podemos decir que la primer componente explica el `r round(pca$eig[1, 2], 2)`%, 
mientras que la segunda componente explica `r round(pca$eig[2, 2], 2)`%. Luego, 
la varianza total explicada por estas dos componentes es 
`r round(pca$eig[2, 3], 2)`%.

**N.** Establezca intuitivamente grupos de variedades similares según su 
cercanía en el plano principal.

A continuacion se presenta la representacion grafica de las variedades en el 
plano principal:

```{r}
pc1 <- pca$ind$coord[, 1]
pc2 <- pca$ind$coord[, 2]
etiquetas <- rownames(pca$ind$coord)
plot(
  pc1,
  pc2, 
  ylab = "2° COMPONENTE PRINCIPAL", 
  xlab = "1° COMPONENTE PRINCIPAL", 
  xlim = c(-6, 6), 
  ylim = c(-6, 6),
  pch = 19
)
abline(h = 0, lty = 3)
abline(v = 0, lty = 3)
text(pc1, pc2, etiquetas, pos = 3, cex = 0.7)
```

Tras observar el grafico podemos decir que encontramos cuatro grupos. 
El primer grupo contiene a MARTINET, CANINO.T y BLANCO. El segundo esta 
conformado por CANINO y TADEO. El tercer grupo esta conformado solamente por 
GABACHET, que se diferencia de todas las variedades. Y el cuarto grupo que se 
compone por el resto de las variedades, que estan ubicadas alrededor del 
comportamiento promedio, es decir, el origen del plano.

**O.** Encuentre e interprete gradientes de las variables originales en el plano
principal en función de sus cargas sobre las dos primeras componentes.

```{r}
plot(pca, choix = "var", title = "Grafico de cargas")
```

**P.** Caracterice los grupos determinados en el inciso (N) según los gradientes
descriptos en (O).

Maria susana.

**Q.** Superponga en la representación del plano principal un MST. Comente al 
respecto, haría algún reagrupamiento ?

```{r}
library(vegan)
```


```{r}
mst <- spantree(matriz_distancia_std_dist)
plot(mst, pca$ind$coord)
# faltan las cp!
```


**R.** Con el software `R` realice el ACP recurriendo a operaciones con 
matrices (decomposición espectral)

Realizando la descomposicion espectral, los autovalores son los siguientes:

```{r}
x <- as.matrix(datos_std)
cor <- cor(x)
autovalores_ <- eigen(cor)

round(autovalores_$values, 1) %>%
  kable(col.names = "AUTOVALORES") %>%
  kable_styling(full_width = FALSE) %>%
  kable_classic_2() 
```

Y los autovectores:

```{r}
round(autovalores_$vectors[, 1:2], 1) %>%
  kable(col.names = c("AUTOVECTOR 1", "AUTOVECTOR 2")) %>%
  kable_styling(full_width = FALSE) %>%
  kable_classic_2()
```

Las nuevas coordenadas de las variedades sobre las componentes halladas:

```{r}
etiquetas <- rownames(pca$var$coord)
p <- as.matrix(autovalores_$vectors)
y <- x %*% p
colnames(y) <- etiquetas
rownames(y) <- datos$VAR

round(y, 1) %>%
  kable() %>%
  kable_classic_2()
```

**S.**  Verifique que con el enfoque Biplot (DVS) llega a los mismos resultados

Valores singulares (d):

```{r}
dvs <- svd(x)

round(dvs$d, 1) %>%
  kable(col.names = "d")%>%
  kable_styling(full_width = FALSE)%>%
  kable_classic_2()
```


Vectores singulares por izquierda (u):

```{r}
colnames(dvs$u) <- etiquetas

dvs$u %>%
  round(1) %>%
  kable() %>%
  kable_classic_2()
```

Vectores singulares por derecha (v):

```{r}
colnames(dvs$v) <- etiquetas

dvs$v %>%
  round(1) %>%
  kable() %>%
  kable_styling(full_width = FALSE) %>%
  kable_classic_2()
```

Las nuevas cordenadas de las variedades halladas a traves de DVS:

```{r}
u <- dvs$u
v <- dvs$v
d <- diag(dvs$d, 13, 13)
ydvs <- u %*% d
colnames(ydvs) <- etiquetas
rownames(ydvs) <- datos$VAR

round(ydvs, 1) %>%
  kable() %>%
  kable_classic_2()
```


**FALTA VERIFICAR QUE SE LLEGA AL MISMO RESULTADO**

1. Autovalores y autovectores coinciden.
2. Graficos de variedades en plano de 2 componentes encimados.

**T.** Obtenga 4 dendogramas ultramétricos según diferentes criterios de 
encadenamiento (SIMPLE, COMPUESTO, UPGMA y WARD).

Criterio de encadenamiento Simple:
```{r}
cluster_min <- hclust(matriz_distancia_std_dist, method = "single")
plot(cluster_min)
```
 
Criterio de encadenamiento Compuesto:
```{r}
cluster_com <- hclust(matriz_distancia_std_dist, method = "complete")
plot(cluster_com)
```

Criterio de encadenamiento UPGMA
```{r}
cluster_upgma <- hclust(matriz_distancia_std_dist, method = "average")
plot(cluster_upgma)
```

Criterio de encadenamiento WARD
```{r}
cluster_ward <- hclust(matriz_distancia_std_dist, method = "ward.D")
plot(cluster_ward)
```


**U.** Asocie a cada uno de los árboles obtenidos en el inciso anterior la 
matriz cofenética correspondiente. Que miden los elementos de estas matrices ?

Encadenamiento Simple:
```{r}
cophenetic_min = cophenetic(cluster_min)
plot(cophenetic_min, matriz_distancia_std_dist)
```


Encadenamiento Compuesto:
```{r}
cophenetic_com = cophenetic(cluster_com)
plot(cophenetic_com, matriz_distancia_std_dist)
```


Encadenamiento UPGMA:
```{r}
cophenetic_upgma = cophenetic(cluster_upgma)
plot(cophenetic_upgma, matriz_distancia_std_dist)
```


Encadenamiento WARD:
```{r}
cophenetic_ward = cophenetic(cluster_ward)
plot(cophenetic_ward, matriz_distancia_std_dist)
```

Los elementos de las matrices anteriores permiten tener una medida de la 
coherencia del criterio de agrupamiento jerarquico

**V.** Cuantifique la concordancia entre la matriz de distancia que dio origen 
a los dendogramas y las 4 matrices cofenéticas. A que conclusión llega ?


Encadenamiento simple
```{r}
cor(cophenetic_min, matriz_distancia_std_dist)
```


Encadenamiento Compuesto
```{r}
cor(cophenetic_com, matriz_distancia_std_dist)
```


Encadenamiento UPGMA
```{r}
cor(cophenetic_upgma, matriz_distancia_std_dist)
```


Encadenamiento WARD
```{r}
cor(cophenetic_ward, matriz_distancia_std_dist)
```

FALTA CONCLUSION

**W.** Existe algún punto de corte sobre el índice de jerarquización del 
dendograma UPGMA que origine los mismos agrupamiento de variedades obtenidos en 
Análisis de Componentes Principales ?

```{r}
cutree(cluster_min, k=6)
```

**X.** Mida el grado de concordancia entre los resultados obtenidos por 
Componentes y por Cluster UPGMA

```{r}
library(ape)
# usar funcion 'nj'
```

**Y.** Halle el dendograma aditivo Neighbor Joining, representa mejor la 
configuración de variedades que el Cluster UPGMA ?