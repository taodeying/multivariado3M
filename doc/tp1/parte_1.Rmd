 

```{r}
datos <- read.csv2(
  here::here("data", "raw", "DAMASCO.csv"), 
  row.names = 1, 
  stringsAsFactors = FALSE
)
```

A) Cuáles son los valores de $n$ y $p$? Cuanto vale y que indica el valor 
$x_{32}$ ? Y el vector $\mathbf{x}_6$ ?

    El valor de $n$ es `r nrow(datos)` y el valor de $p$ es `r ncol(datos)`. El
    valor de $x_{32}$ es `r datos[[3, 2]]` e indica el tamaño de la flor de la 
    observacion 3. El vector $\mathbf{x}_6$ indica la relacion entre el ancho y largo de la 
    hoja.

A) Cómo clasificaría las variables sobre las que se está trabajando ?

    Las variables con las que se esta trabajando son de tipo continuo en todos 
    los casos. Mas aun, todas estan medidas en escala de intervalo.

A) Encuentre el vector de medias y matriz de varianzas-covariancias 
asociados a la tabla de datos.

    Las medias son

    ```{r}
    medias <- datos %>% 
        summarise_all(mean)
    
    kable(medias[1:7], digits = 1) %>%
      kable_styling(font_size = 8, latex_options = "HOLD_position")
    
    kable(medias[8:13], digits = 1) %>%
      kable_styling(font_size = 8, latex_options = "HOLD_position")
    ```

    Calculamos la matriz de covarianza de la siguiente manera
    
    ```{r, echo=TRUE}
    matriz_covarianza <- cov(datos)
    ```
    
    Pero solo mostramos aquellos casos de mayor covarianza, ya que la tabla
    es muy grande como para incluirla en formato pdf.
    
    ```{r}
    d <- as.data.frame(t(combn(colnames(datos), 2)))
    colnames(d) <- c("Variable 1", "Variable 2")
    d$covarianza <- as.vector(as.dist(matriz_covarianza))
    d <- d %>%
      arrange(abs(covarianza))
    ```
    
    ```{r}
    t1 <- head(d, 5)
    t2 <- tail(d, 5)
    
    kable(
      list(t1, t2), 
      col.names = c("Variable 1", "Variable 2", "Covarianza"),
      row.names = FALSE,
      digits = 3,
      align = "c"
    ) %>% 
    kable_styling(font_size = 8, latex_options = "HOLD_position") %>% 
    kable_classic_2() 
    ```
    
    
A) Podría decir cuál y cuáles variables son las más dispersas ?

    Utilizando el coeficiente de variación, podemos decir que las variables más 
    dispersas, en orden decreciente, son:

    ```{r}
    cvs <- datos %>%
        summarise_all(function(x) sd(x) / mean(x)) %>%
        sort(decreasing = TRUE)
    
    kable(cvs[1:7], digits = 2) %>%
      kable_styling(font_size = 8, latex_options = "HOLD_position")
    
    kable(cvs[8:13], digits = 2) %>%
      kable_styling(font_size = 8, latex_options = "HOLD_position")
    ```

    Por lo tanto, podemos concluir que la variable mas dispersa es el peso del
    endocarpio, seguido por el peso de la flor. Si hubieramos observado otra
    medida que depende de la escala de medicion, como por ejemplo el desvio 
    estandar, no hubieramos incluido al peso del endocarpio ya que su valor
    medio (2.4) es mucho mas bajo que el valor medio de otras variables, como 
    por ejemplo tamaño de la flor, largo de la flor, etc.

A) Estandarice las variables por media y desvío. Ahora puede responder al 
inciso (d) ?
    
    ```{r}
    datos_std <- scale(datos)
    ```

    El coeficiente de variacion no existe ya que todas las variables tienen 
    media igual a 0, por lo que no podriamos responder al inciso (d) luego de 
    la estandarizacion. Sin embargo, lo expuesto en el inciso (d) es suficiente
    para concluir sobre que variables tienen mayor y menor dispersion.
    
A) Halle la matriz de correlación. Que variables son las más relacionadas?

    La martiz de correlacion es obtenida con la funcion `cor()`.

    ```{r, echo=TRUE}
    matriz_correlacion <- cor(datos)
    ```

    A continuación una representacion grafica de la matriz de correlacion, la cual permite
    identificar de forma mas sencilla las variables mas relacionadas:      

    ```{r, fig.width = 8, fig.height = 4.5}
    ggcorrplot::ggcorrplot(
        matriz_correlacion,
        method = "circle", 
        type = "upper",
        outline.col = "black",
        ggtheme = ggplot2::theme_gray,
        legend.title = "Correlacion",
        colors = c(tail(colores, 1), "#ffffff", colores[1])
      ) + 
      guides(
        fill = guide_colorbar(barheight = grid::unit(0.75, "npc"))
      )
    ```

A) Pueden dividirse las variables en subgrupos, de modo que las variables 
dentro de un mismo subgrupo tengan elevadas correlaciones entre sí y que las que
se encuentren en subgrupos diferentes tengan bajas correlaciones? Si es así, 
cuáles variables quedan en cada uno de los subgrupos?
    
    A traves del grafico presentado en el inciso anterior resulta sencillo ver
    las agrupaciones de varibales con altas o bajas correlaciones.
    Luego los subgrupos de variables quedan conformados de la siguiente manera:
    * Subgrupos de variables con altas correlaciones: 
    1. Peso, longitud, ancho y espesor del fruto (caracteristicas del fruto)
    1. Peso, longitud y ancho del endocarpio (caracteristicas del endocarpio)
    1. Tamaño de la flor, longitu y ancho del petalo (caracteristicas de la flor)

    *Subgrupo de variables con bajas correlaciones: 
    1. Superficie de la hoja, relacion entre peciolo-limbo y relacion entre longitud
    y ancho de la hoja (caracteristicas de la hoja); tamaño de la flor, longitud y 
    ancho del petalo (caracteristicas de la flor)

    Cabe destacar que la superficie de la hoja y relacion peciolo-limbo presentan
    correlacion muy baja o nula con cualquiera de las otras variables.

A) Encuentre la matriz que mide el grado de similaridad entre las variedades
en función de la distancia euclídea calculada sobre los datos originales.

    ```{r, echo=TRUE}
    matriz_distancia <- dist(datos, method = "euclidean")
    
    ```

A) Podría decir cuales son los tres pares de variedades que presentan mayor 
semejanza?

    
    ```{r}
    d <- as.data.frame(t(combn(rownames(datos), 2)))
      colnames(d) <- c("Variedad 1", "Variedad 2")
      d$distancia <- as.vector(matriz_distancia)
    d <- d %>%
      arrange(distancia)
    kable(
      head(d, 3), 
      col.names = c("Variedad 1", "Variedad 2", "Distancia"),
      row.names = FALSE,
      digits = 3,
      align = "c"
    ) %>% 
    kable_styling(font_size = 8, latex_options = "HOLD_position") %>% 
    kable_classic_2() 
    ```

A) Repita lo realizado en el inciso (h) pero sobre las variables 
estandarizadas por media y desvío estándar. Son las mismas las tres variedades 
más parecidas? Comente al respecto

    ```{r, echo=TRUE}
    matriz_distancia_std <- dist(scale(datos))
    ```

    Con los datos estandarizados, los pares de variedades mas parecidas son

    ```{r}
    d <- as.data.frame(t(combn(rownames(datos), 2)))
      colnames(d) <- c("Variedad 1", "Variedad 2")
      d$distancia <- as.vector(matriz_distancia_std)
    d <- d %>%
      arrange(distancia)
    kable(
      head(d, 3), 
      col.names = c("Variedad 1", "Variedad 2", "Distancia"),
      row.names = FALSE,
      digits = 3,
      align = "c"
    ) %>% 
    kable_styling(font_size = 8, latex_options = "HOLD_position") %>% 
    kable_classic_2() 
    ```

    Podemos ver que los tres pares de variedades mas parecidas son distintos a los
    que vimos en el inciso anterior donde utilizamos los datos sin estandarizar. 
    Esto sucede porque las variables estan medidas en diferentes unidades de 
    medicion, y al utilizar las variables sin estandarizar se le da mayor peso a 
    las que tienen una variabilidad mayor valor en la escala de medida original.

A) Mida el grado de concordancia entre ambas matrices de distancia.

    ```{r}
    concordancia <- cor(matriz_distancia, matriz_distancia_std)
    ```


    ```{r}
    concordancia_df <- data.frame(
        x = as.vector(matriz_distancia),
        y = as.vector(matriz_distancia_std)
    )
    ggplot(concordancia_df) + 
        geom_point(aes(x, y), size = 3, alpha = 0.7, color = "grey30") + 
        labs(
            x = "Distancia sobre datos originales",
            y = "Distancia sobre datos estandarizados"
        )
    ```   

    El grado de concordancia entre las matrices de distancias es
    `r round(concordancia, 3)`.

A) Realice un Análisis de Componentes Principales utilizando de la matriz de
correlaciones.

    ```{r, echo=TRUE}
    pca <- PCA(datos, ncp = 2, graph = FALSE)
    ```
    
    ```{r}
    autovalores_vector <- as.vector(prop.table(pca$eig[, 1]))
    autovalores <- data.frame(
      x = 1:13,
      varianza = autovalores_vector,
      varianza_cum = cumsum(autovalores_vector)
    )
    autovalores$label <- paste0(round(autovalores$varianza * 100), "%")
    
    l <- ggplot(autovalores, aes(x = x)) + 
      geom_hline(yintercept = 1, linetype = "dashed", color = "gray30", alpha = 0.5) + 
      geom_col(aes(y = varianza), fill = colores[1]) +
      geom_line(
        aes(y = varianza_cum), 
        linetype = "dashed",
        size = 1.2, 
        color = "grey30",
        lineend = "round"
      ) + 
      geom_label(
        aes(y = varianza, label = label), 
        nudge_y = 0.025
      ) + 
      scale_x_continuous(
        breaks = 1:13
      ) + 
      scale_y_continuous(
        labels = scales::percent_format()
      ) + 
      labs(
        x = "Componente",
        y = "Varianza"
      ) + 
      theme(
        panel.grid.minor.x = element_blank()
      )
    ```

    Y los autovectores (cargas asociadas a cada componente) son

    ```{r}
    
    variables <- as.data.frame(pca$var$coord)
    variables$etiqueta <- rownames(pca$var$coord)
    variables$x0 <- 0
    variables$y0 <- 0
    
    r <- ggplot(variables) + 
        geom_vline(xintercept = 0, linetype = "dashed", color = "gray30", alpha = 0.5) + 
        geom_hline(yintercept = 0, linetype = "dashed", color = "gray30", alpha = 0.5) +
        geom_circle(
          aes(x0 = x, y0 = y, r = r), 
          linetype = "dashed", color = "gray30", alpha = 0.5,
          data = data.frame(x = 0, y = 0, r = 1)
        ) + 
        geom_segment(
          aes(x = x0, y = y0, xend = Dim.1, yend = Dim.2),
          arrow = arrow(length = unit(0.15, "cm"), type ="closed"),
          size = 1.15,
          color = "grey30",
          show.legend = FALSE
        ) + 
        scale_color_manual(values = colores[c(1, 4)]) + 
        geom_label_repel(
          aes(Dim.1, Dim.2, label = etiqueta), 
          min.segment.length = 0,
          max.overlaps = 30
        ) + 
        lims(x = c(-1, 1), y = c(-1, 1)) + 
        labs(x = "Dimension 1", y = "Dimension 2")
    ```

    ```{r, fig.width = 11, fig.height = 5}
    l + r
    ```

A) Analice los porcentaje de variabilidad explicada por los primeros ejes 
principales.

    Observando los autovalores y el porcentaje de varianza explicada de cada uno, 
    podemos decir que la primer componente explica el `r round(pca$eig[1, 2], 2)`%, 
    mientras que la segunda componente explica `r round(pca$eig[2, 2], 2)`%. Luego, 
    la varianza total explicada por estas dos componentes es 
    `r round(pca$eig[2, 3], 2)`%.

A) Establezca intuitivamente grupos de variedades similares según su 
cercanía en el plano principal.

    A continuacion se presenta la representacion grafica de las variedades en el 
    plano principal:

    ```{r, fig.width = 7, fig.height=4}
    pc1 <- pca$ind$coord[, 1]
    pc2 <- pca$ind$coord[, 2]
    etiquetas <- rownames(pca$ind$coord)
    datos_cp <- as.data.frame(cbind(etiquetas, pc1, pc2))
    colnames(datos_cp) <- c("VARIEDAD","CP1", "CP2")
    
    ggplot(datos_cp) + 
      geom_vline(xintercept = 0, linetype = "dashed", color = "gray30", alpha = 0.5) + 
      geom_hline(yintercept = 0, linetype = "dashed", color = "gray30", alpha = 0.5) +
      geom_point(aes(pc1, pc2), size = 4, color = "gray30") + 
      geom_label_repel(aes(pc1, pc2, label = VARIEDAD), size=2.75) + 
      labs(
        x = "Coordenada Principal 1",
        y = "Coordenada Principal 2"
      )
    ```

    Tras observar el grafico podemos decir que encontramos cuatro grupos. 
    El primer grupo contiene a MARTINET, CANINO.T y BLANCO. El segundo esta 
    conformado por CANINO y TADEO. El tercer grupo esta conformado solamente por 
    GABACHET, que se diferencia de todas las variedades. Y el cuarto grupo que se 
    compone por el resto de las variedades, que estan ubicadas alrededor del 
    comportamiento promedio, es decir, el origen del plano.

A) Encuentre e interprete gradientes de las variables originales en el plano
principal en función de sus cargas sobre las dos primeras componentes.

    Considerando la primer componente, los damascos que tengan flores, frutos y 
    endocarpio grandes se van a ubicar a la derecha del grafico. Con respecto a 
    la segunda componente, los damoscos cuyas hojas sean grandes estarán ubicados 
    en la parte superior del grafico, lo mismo ocurre con aquellos damascos con 
    flores grandes. Los damascos que tengan frutos pequeños, estaran ubicados en
    la parte inferior del grafico. 

    Luego:

    * Damascos con hojas grandes estaran ubicados en el segundo cuadrante del 
    grafico.
    * Damascos con endocarpio y frutos grandes estaran ubicados en el cuarto 
    cuadrante del grafico
    * Damascos con flores grandes estaran ubicados en el primer cuadrante del grafico
   
A) Caracterice los grupos determinados en el inciso (N) según los gradientes
descriptos en (O).

    Grupo 1: es un grupo de variedades caracterizadas por tener flores, endocarpio y
    hojas grandes y frutos medianos.

    Grupo 2: es un grupo caracterizado por tener endoncarpio grande, frutos grandes,
    hojas pequeñas y flores medianas

    Grupo 3: es un fruto caracterizado por tener hojas grandes, frutos y endocarpio
    pequeños y flores medianas. 

A) Superponga en la representación del plano principal un MST. Comente al 
respecto, haría algún reagrupamiento ?

    ```{r, fig.width=6, fig.height=4}
    mst <- spantree(matriz_distancia_std)
    par(mar = c(4.5, 4.5, 1, 1))
    plot(
      mst, 
      pca$ind$coord, 
      xlab = "Componente 1", 
      ylab = "Componente 2",
      type = "t",
      col = "grey30",
      cex = 0.75,
    )
    abline(h = 0, lty = "dashed", col = "grey30")
    abline(v = 0, lty = "dashed", col = "grey30")
    ```

A) Con el software `R` realice el ACP recurriendo a operaciones con 
matrices (decomposición espectral)

    Realizamos la descomposicion espectral de la siguiente manera
  
    ```{r, echo=TRUE}
    X <- as.matrix(scale(datos))
    eig <- eigen(cov(X))
    P <- eig$vectors
    Y_espectral <- X %*% P 
    ```

A)  Verifique que con el enfoque Biplot (DVS) llega a los mismos resultados

    ```{r}
    # Esto es solamente para verificar lo mismo que hizo el profe
    D <- diag(eig$values)
    COV2 <- P %*% D %*% t(P)
    # all.equal(COV2, unname(cov(X)))
    ```

    Primero calculamos las coordenadas en el plano principal mediante el 
    enfoque Biplot.

    ```{r, echo=TRUE}
    dvs <- svd(X)
    U <- dvs$u
    D <- diag(dvs$d)
    Y_dvs <- U %*% D
    ```

    ```{r}
    # hacer acp_1/acp_2 y ves 1 y -1.
    acp_1 <- as.data.frame(Y_espectral[, 1:2])
    acp_2 <- as.data.frame(Y_dvs[, 1:2])
    acp_2$V2 <- -1 * acp_2$V2
    acp_1$metodo <- "Espectral"
    acp_2$metodo <- "DVS"
    acp_ambos <- rbind(acp_1, acp_2)
    ```
    
    Si hacemos la razon entre las dos primeras coordenadas obtenidas mediante descomposicion
    espectral y mediante DVS, vemos que la segunda razon es igual a `-1`. Esto
    se da porque la configuracion de los puntos mediante DVS resulta estar 
    reflejada en el eje X respecto de la configuracion que obutivmos mediante
    la descomposicion espectral. En el siguiente grafico multiplicamos a la 
    segunda componente por `-1` para mostrar mas facilmente la equivalencia de las
    configuraciones obtenidas mediante ambos enfoques.

    ```{r, fig.width = 6, fig.height = 4}
    ggplot(acp_ambos) + 
      geom_vline(xintercept = 0, linetype = "dashed", color = "gray30", alpha = 0.5) + 
      geom_hline(yintercept = 0, linetype = "dashed", color = "gray30", alpha = 0.5) +
      geom_point(aes(V1, V2, size = metodo), alpha = 0.5, color = colores[1]) + 
      scale_size_discrete(name = "Metodo") + 
      labs(
        x = "Coordenada Principal 1",
        y = "Coordenada Principal 2"
      )
    ```

A) Obtenga 4 dendogramas ultramétricos según diferentes criterios de 
encadenamiento (SIMPLE, COMPUESTO, UPGMA y WARD).
    
    ```{r}
    dendograma <- function(cluster, k, title) {
      cluster_min_data <- dendro_data_k(cluster, k = k)
      adjust <- 0.1 * max(cluster_min_data$segments$yend)
      ggplot(cluster_min_data$segments) + 
        geom_segment(
          aes(x = x, y = y, xend = xend, yend = yend, color = as.factor(clust)),
          size = 1.2,
          lineend = "round"
        ) + 
        geom_text(
          aes(x = x, y = y - adjust, label = label, color = as.factor(clust)), 
          data = cluster_min_data$labels
        ) + 
        ylim(
          -0.15 * max(cluster_min_data$segments$yend) , 
          max(cluster_min_data$segments$yend)
        ) + 
        coord_flip() + 
        scale_colour_manual(values = c("grey30", colores)) +
        labs(title = title) +
        theme(
          panel.grid.minor = element_blank(),
          axis.title = element_blank(), 
          axis.text = element_blank(),
          axis.line = element_blank(), 
          axis.ticks = element_blank(),
          plot.title = element_text(hjust = 0.5),
          legend.position = "none"
        )
    }
    ```
    
    ```{r, fig.width = 14, fig.height = 12}
    cluster_min <- hclust(matriz_distancia_std, method = "single")
    ul <- dendograma(cluster_min, 4, title = "Simple")
    
    cluster_com <- hclust(matriz_distancia_std, method = "complete")
    ur <- dendograma(cluster_com, 4, title = "Completo")

    cluster_upgma <- hclust(matriz_distancia_std, method = "average")
    bl <- dendograma(cluster_upgma, 4, title = "UPGMA")
    
    cluster_ward <- hclust(matriz_distancia_std, method = "ward.D")
    br <- dendograma(cluster_ward, 4, title = "Ward")
    
    (ul + ur) / (bl + br)
    ```


A) Asocie a cada uno de los árboles obtenidos en el inciso anterior la 
matriz cofenética correspondiente. Que miden los elementos de estas matrices ?

    ```{r, echo=TRUE}
    cophenetic_min <- cophenetic(cluster_min)
    cophenetic_comp <- cophenetic(cluster_com)
    cophenetic_upgma <- cophenetic(cluster_upgma)
    cophenetic_ward <- cophenetic(cluster_ward)
    ```
    
    Los elementos de las matrices anteriores permiten tener una medida de la 
    coherencia del criterio de agrupamiento jerarquico. Como pudo observase el 
    criterio de encadenamiento UPGMA es el que brinda, para este conjunto de datos, 
    un mejor criterio de agrupamiento. 

A) Cuantifique la concordancia entre la matriz de distancia que dio origen 
a los dendogramas y las 4 matrices cofenéticas. A que conclusión llega ?

    ```{r}
    scatterplot <- function(original, cofenetica, title) {
      data <- as.data.frame(cbind(original, cofenetica))
      corr <- cor(original, cofenetica)
      ggplot(data) +
        geom_point(
          aes(original, cofenetica), 
          size = 3, 
          alpha = 0.7, 
          color = "grey30"
        ) +
        labs(title = paste0(title, " (", round(corr, 3), ")")) + 
        theme(
          plot.title = element_text(hjust = 0.5),
          axis.title = element_blank()
        )
    }
    ```


    ```{r, fig.width = 10, fig.height = 10}
    # ul: upper left
    # br: bottom right... y asi.
    ul <- scatterplot(matriz_distancia_std, cophenetic_min, "Simple")
    ur <- scatterplot(matriz_distancia_std, cophenetic_comp, "Compuesto")
    bl <- scatterplot(matriz_distancia_std, cophenetic_upgma, "UPGMA")
    br <- scatterplot(matriz_distancia_std, cophenetic_ward, "Ward")
    (ul + ur) / (bl + br)
    ```

    Observando las correlaciones obtenidas entre la matriz de distancia y las 
    diferentes matrices cofeneticas puede decirse que la obtenida a traves del 
    metodo de encadenamiento UPGMA es la mas alta. El metodo de encadenamiento
    UPGMA es el que 
    
    
    QUE MAS??

A) Existe algún punto de corte sobre el índice de jerarquización del 
dendograma UPGMA que origine los mismos agrupamiento de variedades obtenidos en 
Análisis de Componentes Principales ?

    Observando el grafico (ver num q le corresponde) podemos ver con los diferentes
    colores que se puede realizar un corte el cual permite obtener los mismos 
    agrupamientos que se obtuvieron a traves del Analisis de Componentes Principales

A) Mida el grado de concordancia entre los resultados obtenidos por 
Componentes y por Cluster UPGMA

    ```{r}
    dist_pca <- dist(pca$ind$coord, method="euclidean")
    cor_pca_upgma <- cor(dist_pca, cophenetic_upgma)%>%
      round(2)
    ```

    Para hallar el grado de concordancia entre los resultados obtenidos por Componentes
    Principales y por CLuster UPGMA se calcula la correlacion entre las matrices de
    distancias, obteniendo una correlacion de `r cor_pca_upgma`. Dicha correlacion es
    alta y esto se debe a que 
    
A) Halle el dendograma aditivo Neighbor Joining, representa mejor la 
configuración de variedades que el Cluster UPGMA ?

    ```{r, fig.height = 4, fig.width=9, fig.cap="Representacion jerarquizada del arbol aditivo Neighbor-Joining."}
    nj <- nj(matriz_distancia_std)
    ggtree::ggtree(nj) + 
            ggtree::geom_tiplab(vjust = 1) + 
            scale_x_reverse() +
            coord_flip() +
            labs(x = "Distancia") + 
            ylim(0, 20) + 
            theme_grey() + 
            theme(
                panel.background = element_rect(),
                panel.grid.minor = element_blank(),
                axis.title.x = element_blank(), 
                axis.title.y = element_text(size = 14),
                axis.text.x = element_blank(),
                axis.text.y = element_text(size = 12), 
                axis.line = element_blank(), 
                axis.ticks.y = element_blank(),
                plot.title = element_text(hjust = 0.5),
                legend.position = "none"
          )
    ```
